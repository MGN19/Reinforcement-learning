{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e33753-3e88-4429-824f-f1670372c545",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009455f1",
   "metadata": {},
   "source": [
    "## Environment - Atari: Adventure\n",
    "\n",
    "**Action Space**\n",
    "\n",
    "The action space is **discrete with 18 possible actions**, consistent across Atari environments using the **ALE (Arcade Learning Environment)** interface. However, for *Adventure*, not all 18 actions have distinct effects. Commonly used actions include:\n",
    "\n",
    "* 0: NOOP (no operation)\n",
    "* 1: FIRE (start the game / interact)\n",
    "* 2: UP\n",
    "* 3: RIGHT\n",
    "* 4: LEFT\n",
    "* 5: DOWN\n",
    "* Other actions (6-17) are combinations or duplicates, often ignored by learning agents for *Adventure*.\n",
    "\n",
    "**Observation Space**\n",
    "\n",
    "The observation is a 3D RGB image representing the current game screen. The shape is (210, 160, 3). For training, this image is often preprocessed:\n",
    "\n",
    "- Converted to grayscale\n",
    "- Resized to 84×84 pixels\n",
    "- Stacked across 4 consecutive frames to capture motion\n",
    "\n",
    "This results in a commonly used processed observation shape of (84, 84, 4).\n",
    "\n",
    "**Rewards**\n",
    "\n",
    "* Rewards are game-specific. For *Adventure*, rewards are sparse and tied to in-game objectives:\n",
    "\n",
    "  * **+1**: Picking up a key, sword, or chalice\n",
    "  * **+1**: Unlocking a castle\n",
    "  * **+1**: Killing a dragon\n",
    "  * **+1**: Returning the chalice to the gold castle (final objective)\n",
    "  * Some events may not give reward despite importance due to original game limitations.\n",
    "\n",
    "**Starting State**\n",
    "\n",
    "* The player begins near the Yellow Castle.\n",
    "* The game state (items, enemies, doors) is initialized with a fixed or semi-random configuration, depending on the game difficulty level (there are 3 game modes: random vs fixed map/object spawns).\n",
    "\n",
    "**Episode Termination**\n",
    "\n",
    "The episode ends when:\n",
    "\n",
    "* The **player successfully returns the chalice to the Gold Castle**.\n",
    "* A **maximum number of steps** is reached (usually 108,000 frames in standard ALE).\n",
    "* The **agent loses all lives**, although *Adventure* does not have traditional lives like platformers.\n",
    "\n",
    "**Version**\n",
    "\n",
    "* V5 (via Gym/MinAtar/ALE integration)\n",
    "\n",
    "**Objective**\n",
    "\n",
    "* Retrieve the **enchanted chalice** and return it to the **Gold Castle**.\n",
    "* Use keys to open castles, avoid or defeat dragons, and explore the game map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8beb20",
   "metadata": {},
   "source": [
    "## Environment Setup Note\n",
    "\n",
    "To use the Atari environments (e.g., `\"ALE/Adventure-v5\"`), install the compatible Gym version:\n",
    "\n",
    "```bash\n",
    "pip install gym==0.26.2 \"gym[atari,accept-rom-license]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21279a87-fc23-4d50-8e38-5c2ecc0939fa",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99dc2066-d2f5-4235-9db2-412581aa30c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T16:00:19.625841Z",
     "iopub.status.busy": "2025-05-31T16:00:19.625417Z",
     "iopub.status.idle": "2025-05-31T16:00:19.630342Z",
     "shell.execute_reply": "2025-05-31T16:00:19.629263Z",
     "shell.execute_reply.started": "2025-05-31T16:00:19.625806Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bcf12e-f12a-4d62-b629-5630a7767f71",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded successfully!\n",
      "Observation shape: (250, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"ALE/Adventure-v5\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(\"Environment loaded successfully!\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c469f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(0, 255, (250, 160, 3), uint8)\n",
      "Action Space: Discrete(18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a04ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(obs)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cbda8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7addd826",
   "metadata": {},
   "source": [
    "Atari environments return raw pixel observations shaped like (210, 160, 3) (RGB), which are:\n",
    "\n",
    "- Large in size (high memory & compute cost)\n",
    "- Full of irrelevant visual details (e.g., score, borders, flicker)\n",
    "- Temporally unstable (a single frame doesn’t show motion clearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8b543",
   "metadata": {},
   "source": [
    "What happens if we don’t preprocess? Training becomes very slow, or fails completely; the network will have a hard time extracting useful features and It may overfit to noise or irrelevant screen elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess frames (resize + grayscale)\n",
    "def preprocess(obs):\n",
    "    obs = Image.fromarray(obs)\n",
    "    obs = obs.convert('L')  # grayscale\n",
    "    obs = obs.resize((84, 84))  # downsample\n",
    "    obs = np.array(obs) / 255.0  # normalize\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7b5ed",
   "metadata": {},
   "source": [
    "| Preprocessing Step                   | Why It's Done                                                  |\n",
    "| ------------------------------------ | -------------------------------------------------------------- |\n",
    "| **Grayscale**                        | Removes color, reducing input from 3 to 1 channel              |\n",
    "| **Resizing (e.g., 210×160 → 84×84)** | Smaller input = faster training and less memory usage          |\n",
    "| **Frame Stacking**                   | Shows motion by combining multiple consecutive frames          |\n",
    "| **Pixel Normalization**              | Stabilizes training (values in `[0, 1]` instead of `[0, 255]`) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d57f4-953c-4f0c-94c5-d5c7a4b7640a",
   "metadata": {},
   "source": [
    "## Solving the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "736f32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # input: 4 stacked frames\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d2d6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_net = DQN(num_actions).to(device)\n",
    "target_net = DQN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "memory = deque(maxlen=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a66deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn():\n",
    "    batch_size = 32\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    update_target_every = 1000\n",
    "\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(5000):\n",
    "        obs, _ = env.reset()\n",
    "        obs = preprocess(obs)\n",
    "        frame_stack = [obs] * 4  # initial 4-frame stack\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            total_steps += 1\n",
    "            state = torch.tensor(np.array(frame_stack), dtype=torch.float32).unsqueeze(0).cuda()\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_obs, reward, done, truncated, _ = env.step(action)\n",
    "            next_obs = preprocess(next_obs)\n",
    "            frame_stack.append(next_obs)\n",
    "\n",
    "            next_state = torch.tensor(np.array(frame_stack), dtype=torch.float32).unsqueeze(0).cuda()\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Training step\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "                states = torch.cat(states)\n",
    "                next_states = torch.cat(next_states)\n",
    "                actions = torch.tensor(actions).unsqueeze(1).cuda()\n",
    "                rewards = torch.tensor(rewards).unsqueeze(1).cuda()\n",
    "                dones = torch.tensor(dones, dtype=torch.uint8).unsqueeze(1).cuda()\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "                next_q_values = target_net(next_states).max(1)[0].detach().unsqueeze(1)\n",
    "                targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            if total_steps % update_target_every == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode} - Total Reward: {total_reward} - Epsilon: {epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7ef0bfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):  \u001b[38;5;66;03m# Max episode length\u001b[39;00m\n\u001b[32m    107\u001b[39m     action = select_action(state, steps_done, policy_net, num_actions)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     next_obs, reward, terminated, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m    110\u001b[39m     next_state, stacked_frames = stack_frames(stacked_frames, next_obs, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/env_checker.py:37\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_step = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233\u001b[39m, in \u001b[36menv_step_passive_checker\u001b[39m\u001b[34m(env, action)\u001b[39m\n\u001b[32m    230\u001b[39m obs, reward, terminated, truncated, info = result\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool8\u001b[49m)):\n\u001b[32m    234\u001b[39m     logger.warn(\n\u001b[32m    235\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    236\u001b[39m     )\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np.bool8)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/numpy/__init__.py:795\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n\u001b[32m    793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "TARGET_UPDATE = 1000\n",
    "MEMORY_SIZE = 100000\n",
    "LR = 1e-4\n",
    "NUM_FRAMES = 500000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocess frames\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (np.stack(state), action, reward, np.stack(next_state), done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # Input is 4 stacked grayscale frames\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0  # normalize pixel values\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    return preprocess(frame).squeeze(0).numpy()\n",
    "\n",
    "def stack_frames(stacked_frames, new_frame, is_new_episode):\n",
    "    frame = preprocess_frame(new_frame)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([frame]*4, maxlen=4)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "    return np.stack(stacked_frames, axis=0), stacked_frames\n",
    "\n",
    "def select_action(state, steps_done, policy_net, num_actions):\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return policy_net(state_tensor).max(1)[1].item()\n",
    "    else:\n",
    "        return random.randrange(num_actions)\n",
    "\n",
    "# Initialize\n",
    "env = gym.make(\"ALE/Adventure-v5\", render_mode=None)\n",
    "num_actions = env.action_space.n\n",
    "policy_net = DQN(num_actions).to(device)\n",
    "target_net = DQN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayBuffer(MEMORY_SIZE)\n",
    "\n",
    "steps_done = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(500):\n",
    "    obs, _ = env.reset()\n",
    "    state, stacked_frames = stack_frames(None, obs, True)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(10000):  # Max episode length\n",
    "        action = select_action(state, steps_done, policy_net, num_actions)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
    "\n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        steps_done += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        # Training step\n",
    "        if len(memory) > BATCH_SIZE:\n",
    "            batch = memory.sample(BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = batch\n",
    "            states = torch.FloatTensor(states).to(device)\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            next_states = torch.FloatTensor(next_states).to(device)\n",
    "            dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "            next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "            expected_q = rewards + (GAMMA * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = nn.functional.mse_loss(q_values.squeeze(), expected_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Plotting learning curve\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN on Adventure-v5\")\n",
    "plt.savefig(\"adventure_dqn_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069705c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
