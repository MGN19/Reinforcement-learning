{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e33753-3e88-4429-824f-f1670372c545",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009455f1",
   "metadata": {},
   "source": [
    "## Environment - Atari: Adventure\n",
    "\n",
    "**Action Space**\n",
    "\n",
    "The action space is **discrete with 18 possible actions**, consistent across Atari environments using the **ALE (Arcade Learning Environment)** interface. However, for *Adventure*, not all 18 actions have distinct effects. Commonly used actions include:\n",
    "\n",
    "* 0: NOOP (no operation)\n",
    "* 1: FIRE (start the game / interact)\n",
    "* 2: UP\n",
    "* 3: RIGHT\n",
    "* 4: LEFT\n",
    "* 5: DOWN\n",
    "* Other actions (6-17) are combinations or duplicates, often ignored by learning agents for *Adventure*.\n",
    "\n",
    "**Observation Space**\n",
    "\n",
    "The observation is a 3D RGB image representing the current game screen. The shape is (210, 160, 3). For training, this image is often preprocessed:\n",
    "\n",
    "- Converted to grayscale\n",
    "- Resized to 84×84 pixels\n",
    "- Stacked across 4 consecutive frames to capture motion\n",
    "\n",
    "This results in a commonly used processed observation shape of (84, 84, 4).\n",
    "\n",
    "**Rewards**\n",
    "\n",
    "* Rewards are game-specific. For *Adventure*, rewards are sparse and tied to in-game objectives:\n",
    "\n",
    "  * **+1**: Picking up a key, sword, or chalice\n",
    "  * **+1**: Unlocking a castle\n",
    "  * **+1**: Killing a dragon\n",
    "  * **+1**: Returning the chalice to the gold castle (final objective)\n",
    "  * Some events may not give reward despite importance due to original game limitations.\n",
    "\n",
    "**Starting State**\n",
    "\n",
    "* The player begins near the Yellow Castle.\n",
    "* The game state (items, enemies, doors) is initialized with a fixed or semi-random configuration, depending on the game difficulty level (there are 3 game modes: random vs fixed map/object spawns).\n",
    "\n",
    "**Episode Termination**\n",
    "\n",
    "The episode ends when:\n",
    "\n",
    "* The **player successfully returns the chalice to the Gold Castle**.\n",
    "* A **maximum number of steps** is reached (usually 108,000 frames in standard ALE).\n",
    "* The **agent loses all lives**, although *Adventure* does not have traditional lives like platformers.\n",
    "\n",
    "**Version**\n",
    "\n",
    "* V5 (via Gym/MinAtar/ALE integration)\n",
    "\n",
    "**Objective**\n",
    "\n",
    "* Retrieve the **enchanted chalice** and return it to the **Gold Castle**.\n",
    "* Use keys to open castles, avoid or defeat dragons, and explore the game map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8beb20",
   "metadata": {},
   "source": [
    "## Environment Setup Note\n",
    "\n",
    "To use the Atari environments (e.g., `\"ALE/Adventure-v5\"`), install the compatible Gym version:\n",
    "\n",
    "```bash\n",
    "pip install gym==0.26.2 \"gym[atari,accept-rom-license]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21279a87-fc23-4d50-8e38-5c2ecc0939fa",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99dc2066-d2f5-4235-9db2-412581aa30c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T16:00:19.625841Z",
     "iopub.status.busy": "2025-05-31T16:00:19.625417Z",
     "iopub.status.idle": "2025-05-31T16:00:19.630342Z",
     "shell.execute_reply": "2025-05-31T16:00:19.629263Z",
     "shell.execute_reply.started": "2025-05-31T16:00:19.625806Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import run_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d3fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3[extra]\n",
      "  Using cached stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from stable-baselines3[extra]) (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.2.6)\n",
      "Collecting torch<3.0,>=2.3 (from stable-baselines3[extra])\n",
      "  Downloading torch-2.7.1-cp310-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.1.1)\n",
      "Collecting pandas (from stable-baselines3[extra])\n",
      "  Downloading pandas-2.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting matplotlib (from stable-baselines3[extra])\n",
      "  Using cached matplotlib-3.10.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting opencv-python (from stable-baselines3[extra])\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting pygame (from stable-baselines3[extra])\n",
      "  Using cached pygame-2.6.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra])\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Collecting tqdm (from stable-baselines3[extra])\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting rich (from stable-baselines3[extra])\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from stable-baselines3[extra]) (0.11.1)\n",
      "Collecting pillow (from stable-baselines3[extra])\n",
      "  Using cached pillow-11.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Collecting filelock (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy>=1.13.3 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra])\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading grpcio-1.73.0-cp310-cp310-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (78.1.1)\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->stable-baselines3[extra])\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->stable-baselines3[extra])\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->stable-baselines3[extra])\n",
      "  Downloading fonttools-4.58.4-cp310-cp310-macosx_10_9_universal2.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->stable-baselines3[extra])\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->stable-baselines3[extra])\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->stable-baselines3[extra])\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->stable-baselines3[extra])\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->stable-baselines3[extra])\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/shhh/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra])\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
      "Downloading torch-2.7.1-cp310-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.73.0-cp310-cp310-macosx_11_0_universal2.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached matplotlib-3.10.3-cp310-cp310-macosx_11_0_arm64.whl (8.0 MB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl (253 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp310-cp310-macosx_10_9_universal2.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached kiwisolver-1.4.8-cp310-cp310-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pillow-11.2.1-cp310-cp310-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Downloading pandas-2.3.0-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached pygame-2.6.1-cp310-cp310-macosx_11_0_arm64.whl (12.4 MB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytz, mpmath, tzdata, tqdm, tensorboard-data-server, sympy, pyparsing, pygame, protobuf, pillow, opencv-python, networkx, mdurl, MarkupSafe, markdown, kiwisolver, grpcio, fsspec, fonttools, filelock, cycler, contourpy, absl-py, werkzeug, pandas, matplotlib, markdown-it-py, jinja2, torch, tensorboard, rich, stable-baselines3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [stable-baselines3]stable-baselines3]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.0 contourpy-1.3.2 cycler-0.12.1 filelock-3.18.0 fonttools-4.58.4 fsspec-2025.5.1 grpcio-1.73.0 jinja2-3.1.6 kiwisolver-1.4.8 markdown-3.8 markdown-it-py-3.0.0 matplotlib-3.10.3 mdurl-0.1.2 mpmath-1.3.0 networkx-3.4.2 opencv-python-4.11.0.86 pandas-2.3.0 pillow-11.2.1 protobuf-6.31.1 pygame-2.6.1 pyparsing-3.2.3 pytz-2025.2 rich-14.0.0 stable-baselines3-2.6.0 sympy-1.14.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 torch-2.7.1 tqdm-4.67.1 tzdata-2025.2 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install \"stable-baselines3[extra]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29653d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import run_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bcf12e-f12a-4d62-b629-5630a7767f71",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f37baad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "env = gym.make('ALE/Adventure-v5')  # remove render_mode in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b7089b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NamespaceNotFound",
     "evalue": "Namespace ALE not found. Have you installed the proper package for ALE?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNamespaceNotFound\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALE/Adventure-v5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/envs/registration.py:689\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/envs/registration.py:533\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    527\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m     )\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/envs/registration.py:399\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/envs/registration.py:362\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check if an env exists in a namespace. If it doesn't, print a helpful error message.\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# First check if the namespace exists\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[43m_check_namespace_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Then check if the name exists\u001b[39;00m\n\u001b[1;32m    365\u001b[0m names: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    366\u001b[0m     env_spec\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m env_spec \u001b[38;5;129;01min\u001b[39;00m registry\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m env_spec\u001b[38;5;241m.\u001b[39mnamespace \u001b[38;5;241m==\u001b[39m ns\n\u001b[1;32m    367\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/envs/registration.py:356\u001b[0m, in \u001b[0;36m_check_namespace_exists\u001b[0;34m(ns)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHave you installed the proper package for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNamespaceNotFound(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNamespaceNotFound\u001b[0m: Namespace ALE not found. Have you installed the proper package for ALE?"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Adventure-v5\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(\"Environment loaded successfully!\")\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c469f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(0, 255, (250, 160, 3), uint8)\n",
      "Action Space: Discrete(18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb062cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'minimal_goexplore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# minimal_go_explore_adventure.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import gym\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mminimal_goexplore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Explorer, Cell  \u001b[38;5;66;03m# from ryanrudes/minimal_goexplore\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      6\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Adventure-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# adventure environment :contentReference[oaicite:1]{index=1}\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'minimal_goexplore'"
     ]
    }
   ],
   "source": [
    "# minimal_go_explore_adventure.py\n",
    "#import gym\n",
    "from minimal_goexplore import Explorer, Cell  # from ryanrudes/minimal_goexplore\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"ALE/Adventure-v5\")  # adventure environment :contentReference[oaicite:1]{index=1}\n",
    "    explorer = Explorer(env, max_steps=100_000, render=False)\n",
    "\n",
    "    # define simple cell representation, e.g., low-res image hash\n",
    "    def cell_fn(obs):\n",
    "        return hash(obs[::20, ::20, :].tobytes())\n",
    "\n",
    "    explorer.cell_fn = cell_fn\n",
    "\n",
    "    # run exploration to build archive\n",
    "    archive = explorer.explore()\n",
    "\n",
    "    # pick best trajectory (e.g., highest score) and replay it\n",
    "    best = max(archive.values(), key=lambda c: c.score)\n",
    "    print(f\"Best score: {best.score}, steps: {len(best.actions)}\")\n",
    "    obs = env.reset()\n",
    "    for a in best.actions:\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a04ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(obs)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cbda8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7addd826",
   "metadata": {},
   "source": [
    "Atari environments return raw pixel observations shaped like (210, 160, 3) (RGB), which are:\n",
    "\n",
    "- Large in size (high memory & compute cost)\n",
    "- Full of irrelevant visual details (e.g., score, borders, flicker)\n",
    "- Temporally unstable (a single frame doesn’t show motion clearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8b543",
   "metadata": {},
   "source": [
    "What happens if we don’t preprocess? Training becomes very slow, or fails completely; the network will have a hard time extracting useful features and It may overfit to noise or irrelevant screen elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f44405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from gym import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "class AtariPreprocessingWrapper(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0.0, high=1.0, shape=(84, 84), dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        obs = Image.fromarray(obs)\n",
    "        obs = obs.convert('L')  # grayscale\n",
    "        obs = obs.resize((84, 84))  # downsample\n",
    "        obs = np.array(obs, dtype=np.float32) / 255.0  # normalize\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f2f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AtariPreprocessingWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92c9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import FrameStack\n",
    "env = FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3f8d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "obs, _ = env.reset()  # obs will be shape: (4, 84, 84)\n",
    "\n",
    "# Plot each stacked frame\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(obs[i], cmap='gray')\n",
    "    axes[i].set_title(f'Frame {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess frames (resize + grayscale)\n",
    "def preprocess(obs):\n",
    "    obs = Image.fromarray(obs)\n",
    "    obs = obs.convert('L')  # grayscale\n",
    "    obs = obs.resize((84, 84))  # downsample\n",
    "    obs = np.array(obs) / 255.0  # normalize\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7b5ed",
   "metadata": {},
   "source": [
    "| Preprocessing Step                   | Why It's Done                                                  |\n",
    "| ------------------------------------ | -------------------------------------------------------------- |\n",
    "| **Grayscale**                        | Removes color, reducing input from 3 to 1 channel              |\n",
    "| **Resizing (e.g., 210×160 → 84×84)** | Smaller input = faster training and less memory usage          |\n",
    "| **Frame Stacking**                   | Shows motion by combining multiple consecutive frames          |\n",
    "| **Pixel Normalization**              | Stabilizes training (values in `[0, 1]` instead of `[0, 255]`) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d57f4-953c-4f0c-94c5-d5c7a4b7640a",
   "metadata": {},
   "source": [
    "## Solving the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055d6f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rl/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "env = gym.make(\"ALE/Adventure-v5\", render_mode=\"rgb_array\")\n",
    "env = ResizeObservation(env, (84, 84))\n",
    "env = GrayScaleObservation(env)\n",
    "env = FrameStack(env, 4)  # frame stacking\n",
    "env = DummyVecEnv([lambda: env])  # SB3 requires VecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa853446",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'wrap_deepmind' from 'stable_baselines3.common.atari_wrappers' (/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/stable_baselines3/common/atari_wrappers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DQN\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_atari_env\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01matari_wrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrap_deepmind\n\u001b[1;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m make_atari_env(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdventureNoFrameskip-v4\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m wrap_deepmind(env, frame_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'wrap_deepmind' from 'stable_baselines3.common.atari_wrappers' (/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/stable_baselines3/common/atari_wrappers.py)"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.atari_wrappers import wrap_deepmind\n",
    "\n",
    "env = make_atari_env(\"AdventureNoFrameskip-v4\", n_envs=1, seed=42)\n",
    "env = wrap_deepmind(env, frame_stack=True)\n",
    "\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1, seed=42, tensorboard_log=\"./logs/\")\n",
    "\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# Rewrap for inference (remove VecEnv)\n",
    "env_for_eval = make_atari_env(\"AdventureNoFrameskip-v4\", n_envs=1, seed=123)\n",
    "env_for_eval = wrap_deepmind(env_for_eval, frame_stack=True)\n",
    "\n",
    "rewards_dqn = run_episodes(model, env_for_eval, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7443542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "\n",
    "env = gym.make(\"ALE/Adventure-v5\", render_mode=\"rgb_array\")\n",
    "env = AtariWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fd0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f4ee01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 14.13GB > 1.34GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.32e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 5296     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.54e-07 |\n",
      "|    n_updates        | 1298     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.14e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.13     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 9159     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.27e-07 |\n",
      "|    n_updates        | 2264     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.13e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 71       |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total_timesteps  | 13563    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.33e-07 |\n",
      "|    n_updates        | 3365     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 963      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 71       |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 15407    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.73e-07 |\n",
      "|    n_updates        | 3826     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 817      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 71       |\n",
      "|    time_elapsed     | 229      |\n",
      "|    total_timesteps  | 16332    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.28e-07 |\n",
      "|    n_updates        | 4057     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 754      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 71       |\n",
      "|    time_elapsed     | 251      |\n",
      "|    total_timesteps  | 18100    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.82e-07 |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 665      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 71       |\n",
      "|    time_elapsed     | 258      |\n",
      "|    total_timesteps  | 18623    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.61e-07 |\n",
      "|    n_updates        | 4630     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 641      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 20498    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2e-06  |\n",
      "|    n_updates        | 5099     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 592      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 301      |\n",
      "|    total_timesteps  | 21308    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.29e-06 |\n",
      "|    n_updates        | 5301     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 640      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 69       |\n",
      "|    time_elapsed     | 367      |\n",
      "|    total_timesteps  | 25585    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.31e-07 |\n",
      "|    n_updates        | 6371     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 647      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 406      |\n",
      "|    total_timesteps  | 28454    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.88e-07 |\n",
      "|    n_updates        | 7088     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 632      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 69       |\n",
      "|    time_elapsed     | 434      |\n",
      "|    total_timesteps  | 30337    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.05e-08 |\n",
      "|    n_updates        | 7559     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 627      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 474      |\n",
      "|    total_timesteps  | 32599    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.59e-07 |\n",
      "|    n_updates        | 8124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 591      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 482      |\n",
      "|    total_timesteps  | 33108    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.16e-07 |\n",
      "|    n_updates        | 8251     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 561      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 493      |\n",
      "|    total_timesteps  | 33671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02e-07 |\n",
      "|    n_updates        | 8392     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 640      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 617      |\n",
      "|    total_timesteps  | 40975    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.54e-08 |\n",
      "|    n_updates        | 10218    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 631      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 644      |\n",
      "|    total_timesteps  | 42883    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.4e-07  |\n",
      "|    n_updates        | 10695    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 627      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 687      |\n",
      "|    total_timesteps  | 45147    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.31e-07 |\n",
      "|    n_updates        | 11261    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 606      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 700      |\n",
      "|    total_timesteps  | 46049    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.49e-08 |\n",
      "|    n_updates        | 11487    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 598      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 728      |\n",
      "|    total_timesteps  | 47869    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.45e-08 |\n",
      "|    n_updates        | 11942    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 579      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 738      |\n",
      "|    total_timesteps  | 48599    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.83e-08 |\n",
      "|    n_updates        | 12124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 566      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 754      |\n",
      "|    total_timesteps  | 49791    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.51e-07 |\n",
      "|    n_updates        | 12422    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 546      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 65       |\n",
      "|    time_elapsed     | 761      |\n",
      "|    total_timesteps  | 50229    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.08e-07 |\n",
      "|    n_updates        | 12532    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 597      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 865      |\n",
      "|    total_timesteps  | 57265    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.41e-07 |\n",
      "|    n_updates        | 14291    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 584      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 880      |\n",
      "|    total_timesteps  | 58397    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.05e-07 |\n",
      "|    n_updates        | 14574    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 564      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 929      |\n",
      "|    total_timesteps  | 61705    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.37e-08 |\n",
      "|    n_updates        | 15401    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 583      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 1008     |\n",
      "|    total_timesteps  | 67465    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.37e-07 |\n",
      "|    n_updates        | 16841    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 578      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 1067     |\n",
      "|    total_timesteps  | 71345    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.37e-08 |\n",
      "|    n_updates        | 17811    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 645      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 1191     |\n",
      "|    total_timesteps  | 79888    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.29e-08 |\n",
      "|    n_updates        | 19946    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 757      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 1356     |\n",
      "|    total_timesteps  | 92021    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.9e-08  |\n",
      "|    n_updates        | 22980    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 788      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 1424     |\n",
      "|    total_timesteps  | 96933    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.57e-08 |\n",
      "|    n_updates        | 24208    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 810      |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 1460     |\n",
      "|    total_timesteps  | 99573    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.25e-05 |\n",
      "|    n_updates        | 24868    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1357538b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\"CnnPolicy\", env, verbose=1, seed=42)\n",
    "\n",
    "timestamps = 100_000 \n",
    "\n",
    "model.learn(total_timesteps=timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530fa739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 0.00\n",
      "Episode 2: Reward = 0.00\n",
      "Episode 3: Reward = 0.00\n",
      "Episode 4: Reward = 0.00\n",
      "Episode 5: Reward = 0.00\n",
      "Episode 6: Reward = 0.00\n",
      "Episode 7: Reward = 0.00\n",
      "Episode 8: Reward = 0.00\n",
      "Episode 9: Reward = 0.00\n",
      "Episode 10: Reward = 0.00\n",
      "Episode 11: Reward = 0.00\n",
      "Episode 12: Reward = 0.00\n",
      "Episode 13: Reward = 0.00\n",
      "Episode 14: Reward = 0.00\n",
      "Episode 15: Reward = 0.00\n",
      "Episode 16: Reward = 0.00\n",
      "Episode 17: Reward = 0.00\n",
      "Episode 18: Reward = 0.00\n",
      "Episode 19: Reward = 0.00\n",
      "Episode 20: Reward = 0.00\n",
      "Episode 21: Reward = 0.00\n",
      "Episode 22: Reward = 0.00\n",
      "Episode 23: Reward = 0.00\n",
      "Episode 24: Reward = 0.00\n",
      "Episode 25: Reward = 0.00\n",
      "Episode 26: Reward = 0.00\n",
      "Episode 27: Reward = 0.00\n",
      "Episode 28: Reward = 0.00\n",
      "Episode 29: Reward = 0.00\n",
      "Episode 30: Reward = 0.00\n",
      "Episode 31: Reward = 0.00\n",
      "Episode 32: Reward = 0.00\n",
      "Episode 33: Reward = 0.00\n",
      "Episode 34: Reward = 0.00\n",
      "Episode 35: Reward = 0.00\n",
      "Episode 36: Reward = 0.00\n",
      "Episode 37: Reward = 0.00\n",
      "Episode 38: Reward = 0.00\n",
      "Episode 39: Reward = 0.00\n",
      "Episode 40: Reward = 0.00\n",
      "Episode 41: Reward = 0.00\n",
      "Episode 42: Reward = 0.00\n",
      "Episode 43: Reward = 0.00\n",
      "Episode 44: Reward = 0.00\n",
      "Episode 45: Reward = 0.00\n",
      "Episode 46: Reward = 0.00\n",
      "Episode 47: Reward = 0.00\n",
      "Episode 48: Reward = 0.00\n",
      "Episode 49: Reward = 0.00\n",
      "Episode 50: Reward = 0.00\n",
      "Episode 51: Reward = 0.00\n",
      "Episode 52: Reward = 0.00\n",
      "Episode 53: Reward = 0.00\n",
      "Episode 54: Reward = 0.00\n",
      "Episode 55: Reward = 0.00\n",
      "Episode 56: Reward = 0.00\n",
      "Episode 57: Reward = 0.00\n",
      "Episode 58: Reward = 0.00\n",
      "Episode 59: Reward = 0.00\n",
      "Episode 60: Reward = 0.00\n",
      "Episode 61: Reward = 0.00\n",
      "Episode 62: Reward = 0.00\n",
      "Episode 63: Reward = 0.00\n",
      "Episode 64: Reward = 0.00\n",
      "Episode 65: Reward = 0.00\n",
      "Episode 66: Reward = 0.00\n",
      "Episode 67: Reward = 0.00\n",
      "Episode 68: Reward = 0.00\n",
      "Episode 69: Reward = 0.00\n",
      "Episode 70: Reward = 0.00\n",
      "Episode 71: Reward = 0.00\n",
      "Episode 72: Reward = 0.00\n",
      "Episode 73: Reward = 0.00\n",
      "Episode 74: Reward = 0.00\n",
      "Episode 75: Reward = 0.00\n",
      "Episode 76: Reward = 0.00\n",
      "Episode 77: Reward = 0.00\n",
      "Episode 78: Reward = 0.00\n",
      "Episode 79: Reward = 0.00\n",
      "Episode 80: Reward = 0.00\n",
      "Episode 81: Reward = 0.00\n",
      "Episode 82: Reward = 0.00\n",
      "Episode 83: Reward = 0.00\n",
      "Episode 84: Reward = 0.00\n",
      "Episode 85: Reward = 0.00\n",
      "Episode 86: Reward = 0.00\n",
      "Episode 87: Reward = 0.00\n",
      "Episode 88: Reward = 0.00\n",
      "Episode 89: Reward = 0.00\n",
      "Episode 90: Reward = 0.00\n",
      "Episode 91: Reward = 0.00\n",
      "Episode 92: Reward = 0.00\n",
      "Episode 93: Reward = 0.00\n",
      "Episode 94: Reward = 0.00\n",
      "Episode 95: Reward = 0.00\n",
      "Episode 96: Reward = 0.00\n",
      "Episode 97: Reward = 0.00\n",
      "Episode 98: Reward = 0.00\n",
      "Episode 99: Reward = 0.00\n",
      "Episode 100: Reward = 0.00\n",
      "Episode 101: Reward = 0.00\n",
      "Episode 102: Reward = 0.00\n",
      "Episode 103: Reward = 0.00\n",
      "Episode 104: Reward = 0.00\n",
      "Episode 105: Reward = 0.00\n",
      "Episode 106: Reward = 0.00\n",
      "Episode 107: Reward = 0.00\n",
      "Episode 108: Reward = 0.00\n",
      "Episode 109: Reward = 0.00\n",
      "Episode 110: Reward = 0.00\n",
      "Episode 111: Reward = 0.00\n",
      "Episode 112: Reward = 0.00\n",
      "Episode 113: Reward = 0.00\n",
      "Episode 114: Reward = 0.00\n",
      "Episode 115: Reward = 0.00\n",
      "Episode 116: Reward = 0.00\n",
      "Episode 117: Reward = 0.00\n",
      "Episode 118: Reward = 0.00\n",
      "Episode 119: Reward = 0.00\n",
      "Episode 120: Reward = 0.00\n",
      "Episode 121: Reward = 0.00\n",
      "Episode 122: Reward = 0.00\n",
      "Episode 123: Reward = 0.00\n",
      "Episode 124: Reward = 0.00\n",
      "Episode 125: Reward = 0.00\n",
      "Episode 126: Reward = 0.00\n",
      "Episode 127: Reward = 0.00\n",
      "Episode 128: Reward = 0.00\n",
      "Episode 129: Reward = 0.00\n",
      "Episode 130: Reward = 0.00\n",
      "Episode 131: Reward = 0.00\n",
      "Episode 132: Reward = 0.00\n",
      "Episode 133: Reward = 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 3\u001b[0m rewards_dqn \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Reinforcement-learning/utils.py:86\u001b[0m, in \u001b[0;36mrun_episodes\u001b[0;34m(model, env, n_episodes)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     85\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 86\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     88\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/core.py:595\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/core.py:560\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    559\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/stable_baselines3/common/atari_wrappers.py:112\u001b[0m, in \u001b[0;36mEpisodicLifeEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtariStepReturn:\n\u001b[0;32m--> 112\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_real_done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# check current lives, make loss of life terminal,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# then update lives to handle bonus lives\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/stable_baselines3/common/atari_wrappers.py:178\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    176\u001b[0m terminated \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m--> 178\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/shhh/lib/python3.10/site-packages/ale_py/env.py:305\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    303\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 305\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    308\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "\n",
    "rewards_dqn = run_episodes(model, env, n_episodes=episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d223d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(model, env, n_episodes=10):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward[0]  # reward is a vector (even if batch size = 1)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b2cc830",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "\n",
    "rewards_dqn = run_episodes(model, env, n_episodes=episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "158cb643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards_dqn)\n",
    "plt.title(\"DQN Agent Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73564c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.1+2750686)\n",
      "[Powered by Stella]\n",
      "Game console created:\n",
      "  ROM file:  /opt/anaconda3/envs/shhh/lib/python3.10/site-packages/ale_py/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1750247406\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface, roms\n",
    "\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(roms.get_rom_path(\"breakout\"))\n",
    "ale.reset_game()\n",
    "\n",
    "reward = ale.act(0)  # noop\n",
    "screen_obs = ale.getScreenRGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97de11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)  # unnecessary but helpful for IDEs\n",
    "\n",
    "env = gym.make('ALE/Adventure-v5', render_mode=\"human\")  # remove render_mode in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b98ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gymnasium\n",
      "Version: 1.1.1\n",
      "Summary: A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).\n",
      "Home-page: https://farama.org\n",
      "Author: \n",
      "Author-email: Farama Foundation <contact@farama.org>\n",
      "License: MIT License\n",
      "Location: /opt/anaconda3/envs/shhh/lib/python3.10/site-packages\n",
      "Requires: cloudpickle, farama-notifications, numpy, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28012aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium.wrappers.frame_stack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AtariPreprocessing\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframe_stack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FrameStack  \u001b[38;5;66;03m# ✅ Correct import\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_env\u001b[39m(env_name, ocatari\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_name, obs_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m, frameskip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, full_action_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium.wrappers.frame_stack'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "def make_env(env_name, ocatari=False):\n",
    "    env = gym.make(env_name, obs_type='rgb', frameskip=4, full_action_space=True)\n",
    "    env = AtariPreprocessing(env, grayscale_obs=True, scale_obs=True)\n",
    "    env = FrameStack(env, 4)\n",
    "    if ocatari:\n",
    "        from oc_atari import OCAtariWrapper\n",
    "        env = OCAtariWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97659a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, rainbow=False):\n",
    "        self.state_shape = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.rainbow = rainbow\n",
    "        self.q_net = build_model(self.state_shape, self.n_actions, rainbow).to(self.device)\n",
    "        self.target_net = build_model(self.state_shape, self.n_actions, rainbow).to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=1e-4)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.eps_min = 0.1\n",
    "        self.eps_decay = 1e-6\n",
    "        self.update_freq = 1000\n",
    "        self.learn_step = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        state_v = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        q = self.q_net(state_v)\n",
    "        return int(q.argmax().item())\n",
    "\n",
    "    def push(self, s, a, r, ns, d):\n",
    "        self.memory.append((s, a, r, ns, d))\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size: return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, nxts, dones = zip(*batch)\n",
    "        # preprocessing omitted...\n",
    "        # compute current & target Q\n",
    "        # use Double Q if rainbow\n",
    "        # loss = ...\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.learn_step % self.update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.learn_step += 1\n",
    "        self.epsilon = max(self.eps_min, self.epsilon - self.eps_decay)\n",
    "\n",
    "    def save(self, path): torch.save(self.q_net.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c52a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def build_model(input_shape, n_actions, rainbow=False):\n",
    "    c, h, w = input_shape\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(c, 32, 8, 4), nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.fc = nn.Sequential(nn.Linear(self._conv_out(h, w), 512), nn.ReLU())\n",
    "            if rainbow:\n",
    "                # distributional output with atoms, noisy-net, dueling...\n",
    "                pass\n",
    "            else:\n",
    "                self.head = nn.Linear(512, n_actions)\n",
    "        def _conv_out(self, h, w):\n",
    "            o = self.conv(torch.zeros(1, c, h, w))\n",
    "            return int(o.shape[1])\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x / 255.)\n",
    "            x = self.fc(x)\n",
    "            return self.head(x)\n",
    "    return Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f065b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdqn_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DQNAgent\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALE/Adventure-v5\u001b[39m\u001b[38;5;124m'\u001b[39m, use_rainbow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wrappers'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def train(env_name='ALE/Adventure-v5', use_rainbow=False):\n",
    "    env = make_env(env_name)\n",
    "    agent = DQNAgent(env, rainbow=use_rainbow)\n",
    "    n_frames = 10_000_000\n",
    "    state = env.reset()[0]\n",
    "    for frame_idx in range(1, n_frames+1):\n",
    "        action = agent.select_action(state)\n",
    "        nxt, reward, done, _, = env.step(action)\n",
    "        agent.push(state, action, reward, nxt, done)\n",
    "        state = nxt\n",
    "        agent.update()\n",
    "        if done:\n",
    "            state = env.reset()[0]\n",
    "        if frame_idx % 100_000 == 0:\n",
    "            agent.save(f'ckpt_{frame_idx}.pth')\n",
    "    env.close()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train(use_rainbow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "736f32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # input: 4 stacked frames\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d2d6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_net = DQN(num_actions).to(device)\n",
    "target_net = DQN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "memory = deque(maxlen=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a66deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn():\n",
    "    batch_size = 32\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    update_target_every = 1000\n",
    "\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(5000):\n",
    "        obs, _ = env.reset()\n",
    "        obs = preprocess(obs)\n",
    "        frame_stack = [obs] * 4  # initial 4-frame stack\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            total_steps += 1\n",
    "            state = torch.tensor(np.array(frame_stack), dtype=torch.float32).unsqueeze(0).cuda()\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_obs, reward, done, truncated, _ = env.step(action)\n",
    "            next_obs = preprocess(next_obs)\n",
    "            frame_stack.append(next_obs)\n",
    "\n",
    "            next_state = torch.tensor(np.array(frame_stack), dtype=torch.float32).unsqueeze(0).cuda()\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Training step\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "                states = torch.cat(states)\n",
    "                next_states = torch.cat(next_states)\n",
    "                actions = torch.tensor(actions).unsqueeze(1).cuda()\n",
    "                rewards = torch.tensor(rewards).unsqueeze(1).cuda()\n",
    "                dones = torch.tensor(dones, dtype=torch.uint8).unsqueeze(1).cuda()\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "                next_q_values = target_net(next_states).max(1)[0].detach().unsqueeze(1)\n",
    "                targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            if total_steps % update_target_every == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode} - Total Reward: {total_reward} - Epsilon: {epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7ef0bfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):  \u001b[38;5;66;03m# Max episode length\u001b[39;00m\n\u001b[32m    107\u001b[39m     action = select_action(state, steps_done, policy_net, num_actions)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     next_obs, reward, terminated, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m    110\u001b[39m     next_state, stacked_frames = stack_frames(stacked_frames, next_obs, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/env_checker.py:37\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_step = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233\u001b[39m, in \u001b[36menv_step_passive_checker\u001b[39m\u001b[34m(env, action)\u001b[39m\n\u001b[32m    230\u001b[39m obs, reward, terminated, truncated, info = result\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool8\u001b[49m)):\n\u001b[32m    234\u001b[39m     logger.warn(\n\u001b[32m    235\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    236\u001b[39m     )\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np.bool8)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/rl/lib/python3.11/site-packages/numpy/__init__.py:795\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n\u001b[32m    793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "TARGET_UPDATE = 1000\n",
    "MEMORY_SIZE = 100000\n",
    "LR = 1e-4\n",
    "NUM_FRAMES = 500000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocess frames\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (np.stack(state), action, reward, np.stack(next_state), done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # Input is 4 stacked grayscale frames\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0  # normalize pixel values\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    return preprocess(frame).squeeze(0).numpy()\n",
    "\n",
    "def stack_frames(stacked_frames, new_frame, is_new_episode):\n",
    "    frame = preprocess_frame(new_frame)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([frame]*4, maxlen=4)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "    return np.stack(stacked_frames, axis=0), stacked_frames\n",
    "\n",
    "def select_action(state, steps_done, policy_net, num_actions):\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return policy_net(state_tensor).max(1)[1].item()\n",
    "    else:\n",
    "        return random.randrange(num_actions)\n",
    "\n",
    "# Initialize\n",
    "env = gym.make(\"ALE/Adventure-v5\", render_mode=None)\n",
    "num_actions = env.action_space.n\n",
    "policy_net = DQN(num_actions).to(device)\n",
    "target_net = DQN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayBuffer(MEMORY_SIZE)\n",
    "\n",
    "steps_done = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(500):\n",
    "    obs, _ = env.reset()\n",
    "    state, stacked_frames = stack_frames(None, obs, True)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(10000):  # Max episode length\n",
    "        action = select_action(state, steps_done, policy_net, num_actions)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
    "\n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        steps_done += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        # Training step\n",
    "        if len(memory) > BATCH_SIZE:\n",
    "            batch = memory.sample(BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = batch\n",
    "            states = torch.FloatTensor(states).to(device)\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "            rewards = torch.FloatTensor(rewards).to(device)\n",
    "            next_states = torch.FloatTensor(next_states).to(device)\n",
    "            dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "            next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "            expected_q = rewards + (GAMMA * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = nn.functional.mse_loss(q_values.squeeze(), expected_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Plotting learning curve\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN on Adventure-v5\")\n",
    "plt.savefig(\"adventure_dqn_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069705c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shhh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
